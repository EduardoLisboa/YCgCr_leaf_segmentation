{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Making the necessary imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from statistics import mean, stdev\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Disable scientific notations for clarity\n",
    "\n",
    "With the parameter \"__supress__\" set to __True__ we make sure to print floating point numbers using fixed floating\n",
    "points."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define useful variables\n",
    "\n",
    "These variables will be used for saving the ploted data. They must be the same used in the __CNN_Model__ notebook."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 25\n",
    "model_version = 3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Read the CSV file containing the labels of the images"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Store the important data in variables\n",
    "\n",
    "The data we are using is:\n",
    "- \"__id__\": the name of the correponding image file.\n",
    "- \"__predominant stress__\": the number that determines if a leaf is healthy or not and the severity of the disease.\n",
    "This value ranges from 0 (healthy) to 4 (very diseased)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_id = data['id'].tolist()\n",
    "data_stress = data['predominant_stress'].tolist()\n",
    "\n",
    "print(data_id)\n",
    "print(data_stress)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create the validation list from the CSV data\n",
    "\n",
    "We go through all testing images and check their correspondent __id__ in the data we extracted from the CSV.\n",
    "\n",
    "When an item is found, we check its __predominant stress__ value and if it's 0, it's a healthy leaf and we add to the\n",
    "validation list \"0\", but if it's greater than 0 it's a diseased leaf and we add to the validation list \"1\"."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "validation = list()\n",
    "for file in os.listdir(os.path.join('Dataset/testing', '')):\n",
    "    split = file.split('.')\n",
    "    file_id = split[0]\n",
    "\n",
    "    if int(file_id) in data_id:\n",
    "        id_index = data_id.index(int(file_id))\n",
    "        validation.append(0 if data_stress[id_index] == 0 else 1)\n",
    "\n",
    "print(validation)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the model\n",
    "\n",
    "This can be a model you created by fiddling with the __CNN_Model__ notebook or can be the provided __best_model.h5__\n",
    "model.\n",
    "\n",
    "Either case, make sure to change the path for the __.h5__ file according to your needs."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = load_model(f'Models/Batch Size {batch_size} - Epochs {epochs} - Model v{model_version}.h5')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Make the predictions\n",
    "\n",
    "Here we set a *__class_names__* variable that stores the name of the classes to identify.\n",
    "\n",
    "Then we read through all the images in the test folder, loading it and setting its size to be the same that we used to\n",
    "train our model, then converting this loaded image to an array and expanding its dimensions.\n",
    "\n",
    "The \"*__expand_dims__*\" method essentially creates a batch. What it does is adding a new dimension to the array in a\n",
    "given axis. In this case, it is creating a new dimension on the \"**image_array**\" in its axis \"**0**\".\n",
    "\n",
    "Then we use our loaded model to make a prediction on that image saving it on the **preds** variable.\n",
    "\n",
    "To compute the score, we use the *__softmax__* function in the first element of the **preds** variable, which contains\n",
    "the percentage of chance our model calculated for the image being of each class. The softmax is going to normalize the\n",
    "array and make the sum of its elements be **1**.\n",
    "\n",
    "Finally we use the *__argmax__* method, which returns the index of the largest element in the **score** list and we\n",
    "check with our **class_names** variable if this index is 0, corresponding to \"**healthy**\" or if it's 1, corresponding\n",
    "to \"**diseased**\", adding 0 or 1 to the **predictions** list, respectively."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_names = ['diseased', 'healthy']\n",
    "predictions = list()\n",
    "for file in os.listdir(os.path.join('Dataset/testing', '')):\n",
    "    img_path = os.path.join('Dataset/testing', file)\n",
    "    img = keras.preprocessing.image.load_img(img_path, target_size=(512, 512))\n",
    "    img_array = keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0)  # Create a batch\n",
    "\n",
    "    preds = model.predict(img_array)\n",
    "    score = tf.nn.softmax(preds[0])\n",
    "    predictions.append(0 if class_names[np.argmax(score)] == 'healthy' else 1)\n",
    "\n",
    "print(predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Make the predictions\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calculate metrics scores\n",
    "\n",
    "For the metrics, we are using four methods of evaluation:\n",
    "- *__Accuracy__*: $\\frac{rights}{total}$, which is the fraction of correct predictions.\n",
    "- *__Precision__*: $\\frac{tp}{(tp + fp)}$, where $tp$ is the number of true positives and $fp$ the number of false\n",
    "positives.\n",
    "- *__Recall__*: $\\frac{tp}{(tp + fn)}$, where $tp$ is the number of true positives and $fn$ the number of false negatives.\n",
    "- *__F1__*: $\\frac{2 * (precision * recall)}{(precision + recall)}$, which is basically a weighted average of\n",
    "**precision** and **recall** scores.\n",
    "\n",
    "After these values are calculated, we calculate the *__mean__* of all metrics and their *__standard deviation__*."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(validation, predictions)\n",
    "precision = precision_score(validation, predictions, average='binary')\n",
    "recall = recall_score(validation, predictions, average='binary')\n",
    "f1 = f1_score(validation, predictions, average='binary')\n",
    "\n",
    "mean = mean([accuracy, precision, recall, f1])\n",
    "std_dev = stdev([accuracy, precision, recall, f1])\n",
    "\n",
    "print(f'Batch Size - {batch_size} | Epochs - {epochs} | Model v{model_version}')\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1: {f1:.4f}')\n",
    "print(f'Mean: {mean:.4f}')\n",
    "print(f'Standard Deviation: {std_dev:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save metrics to file\n",
    "\n",
    "Finally, we save all these metrics to a TXT file, which is updated every time new tests are made."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('Accuracy Scores.txt', 'a') as file:\n",
    "    file.write(f'Batch Size - {batch_size} | Epochs - {epochs} | Model v{model_version}\\n')\n",
    "    file.write(f'Accuracy: {accuracy:.4f}\\n')\n",
    "    file.write(f'Precision: {precision:.4f}\\n')\n",
    "    file.write(f'Recall: {recall:.4f}\\n')\n",
    "    file.write(f'F1: {f1:.4f}\\n')\n",
    "    file.write(f'Mean: {mean:.4f}\\n')\n",
    "    file.write(f'Standard Deviation: {std_dev:.4f}\\n\\n')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}