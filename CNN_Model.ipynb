{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Making the necessary imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setting the batch size, number of epochs and target size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 25\n",
    "image_size = (512, 512)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating training and validation images data\n",
    "\n",
    "Here we use the function *image_dataset_from_directory* setting the training data from the *Dataset/training* folder.\n",
    "\n",
    "We set the *subset* as \"*training*\" and *validation_split* as \"*0.2*\", in other words, from all the images inside the\n",
    "*training* folder, 80% will be used in training and 20% in validation.\n",
    "\n",
    "The *image_size* and *batch_size* are the same as defined above."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_ds = keras.preprocessing.image_dataset_from_directory(\n",
    "    'Dataset/training',\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=1337,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical'\n",
    ")\n",
    "val_ds = keras.preprocessing.image_dataset_from_directory(\n",
    "    'Dataset/training',\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=1337,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Print the class names and set the number of classes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class_names = train_ds.class_names\n",
    "num_classes = len(class_names)\n",
    "print(f'Class names: {class_names}')\n",
    "print(f'Number of classes: {num_classes}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prepare data augmentation and normalization layers\n",
    "\n",
    "The dataset used is very small, only has 1216 images, the model might have a tendency to overfit. To prevent that from\n",
    "happening, we preprocess the images to increase the amount images and the variety that the model is processing.\n",
    "\n",
    "Our image has RGB coeficients ranging from 0 to 255 and that is too large for our model to process. In order to simplify\n",
    "this, we use a normalization layer which, by rescaling with a factor of 1./255, it changes all pixels values to a range\n",
    "of 0 to 1."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.experimental.preprocessing.RandomFlip('horizontal', input_shape=(image_size[0], image_size[1], 3)),\n",
    "        keras.layers.experimental.preprocessing.RandomFlip('vertical', input_shape=(image_size[0], image_size[1], 3)),\n",
    "        keras.layers.experimental.preprocessing.RandomRotation(0.1),\n",
    "        keras.layers.experimental.preprocessing.RandomZoom(0.1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "normalization_layer = keras.layers.experimental.preprocessing.Rescaling(1./255)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Prepare data augmentation and normalization layers\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prefetching, Shuffling and Caching\n",
    "\n",
    "The *prefetch* method separates the moment when the data is created from the moment when the data is consumed, basically\n",
    "it uses a separate thread and an internal buffer to prefetch entry data elements before the moment they are consumed.\n",
    "\n",
    "The *shuffle* method randomly shuffles elements from the dataset.\n",
    "\n",
    "And the *cache* method stores the dataset in memory or local storage, preventing unnecessary operations (like file\n",
    "openings and data readings) from happening during each epoch."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating the model\n",
    "\n",
    "Here we create the structure of the model, being the first two layers the **data augmentation** and **normalization**\n",
    "layers we previously created, followed by two **2D convolutional** layers with **16** output filters, kernel size of\n",
    "__3__, padding \"*same*\" to guarantee that the output has the same width and height as the input and activation **ReLU**\n",
    "(Rectified Linear Units).\n",
    "\n",
    "Then a **2D max pooling** layer, which downsamples the input along its spacial dimensions (height and width) by taking\n",
    "the maximum value over an input window for each channel of the input. The size of this input window is defined by a\n",
    "parameter called \"*pool_size*\" and we used its default value, which is **(2, 2)**.\n",
    "\n",
    "After this we have three more **2D convolutional** layers interspersed with three **2D max pooling** layers. These\n",
    "layers work the same way as described above, the only change is in the convolutional layers where the new output filters\n",
    "are 32, 64 and 128.\n",
    "\n",
    "We then have a **dropout** layer, that randomly sets input units to 0 with a frequency of *__0.2__* at each step during\n",
    "training time. This is used to prevent overfitting.\n",
    "\n",
    "Then comes a **flatten** layer, which flattens the input, transforming all its channels into a single array.\n",
    "\n",
    "And lastly, three **dense** layers. The first **128** units and activation \"**ReLU**\", the second with **64** units and\n",
    "the same activation as the previous and the last layer, the one that outputs the prediction, has the **number of\n",
    "classes** as the number of units, in our case __2__, and activation is \"**softmax**\".\n",
    "\n",
    "#### ReLU and Softmax\n",
    "\n",
    "ReLU is defined by the formula: $ReLU(x) = max(0, x)$\n",
    "\n",
    "And Softmax is defined by the formula: $Sofmax(x_i) = \\frac{exp(x_i)}{\\sum_j exp(x_j)}$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    data_augmentation,\n",
    "    normalization_layer,\n",
    "    Conv2D(16, 3, padding='same', activation='relu'),\n",
    "    Conv2D(16, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(128, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Dropout(0.2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model_version = 4"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Show the model summary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set the optimizer and compile the model\n",
    "\n",
    "For the optimizer we used *__Adam__*, which is an algorithm based on other famous algorithm called \"**Stochastic Gradient\n",
    "Descent**\", with a *learning rate* of *__0.001__*.\n",
    "\n",
    "To compile our model, we use the method **compile** with 3 parameters:\n",
    "- *__optimizer__*: the Adam optimizer\n",
    "- *__loss__*: Categorical Crossentropy, which is a Softmax function followed by the cross-entropy loss function, defined\n",
    "by $CE = -\\sum_{i}^Ct_ilog(s_i)$, where $t_i$ is the groundtruth and $s_i$ is the CNN score for each class.\n",
    "- *__metrics__*: Accuracy, which is the metric we want the model to evaluate during training and testing."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "opt = Adam(learning_rate=0.001)\n",
    "model.compile(\n",
    "    optimizer=opt,\n",
    "    loss=keras.losses.categorical_crossentropy,\n",
    "    metrics=['accuracy']\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set checkpoints and early stopping\n",
    "\n",
    "We use the callback functions *__ModelCheckpoint__* and *__EarlyStopping__* to save our progress at each improving step\n",
    "of the model training by saving a version of the current model in a specified location in our computer, as well as, if\n",
    "there are no improvements in the monitored metric (in this case **accuracy**, defined in the \"*monitor*\" parameter) in\n",
    "**20** epochs (defined in the \"*patience*\" parameter), the training of the model will be stopped."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(f'Models/ModelCheckpoint-{batch_size}-{epochs}-v{model_version}.h5',\n",
    "                             monitor='accuracy', verbose=1, save_best_only=True,\n",
    "                             save_weights_only=False, mode='auto', save_freq=1)\n",
    "early = EarlyStopping(monitor='accuracy', min_delta=0, patience=20, verbose=1, mode='auto')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train the model\n",
    "\n",
    "Here we start our model training.\n",
    "\n",
    "For the parameters we used:\n",
    "- *__train_ds__*: the training dataset we prepared in the beginning.\n",
    "- *__validation_data__*: *__val_ds__*, the validation dataset we prepared in the beginning.\n",
    "- *__epochs__*: The *__epochs__* variable we set in the beginning.\n",
    "- *__callbacks__*: The *__checkpoint__* and *__early stopping__* functions we just made.\n",
    "\n",
    "We save our training in the *__hist__* variable so we can plot the training and validation data.\n",
    "\n",
    "We also print out the elapsed time just for a better perception of the process."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "hist = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs,\n",
    "    callbacks=[checkpoint, early]\n",
    ")\n",
    "total_time = (time.time() - start) / 60\n",
    "print(f'\\nElapsed time: {total_time:.2f} minutes')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plot the training data\n",
    "\n",
    "Here we store our model's training data in variables just for clarity.\n",
    "\n",
    "We plot two graphs, the first with the training and validation accuracy and the second with the training and validation\n",
    "loss.\n",
    "\n",
    "We then save this ploted graphs to a PNG file inside the _Plots_ folder with its name saying the **batch size** and\n",
    "**epochs** used as well as which **model structure version** we are using. The latter only need to be changed if the\n",
    "structure in \"**Creating the model**\" is changed. This variable is right below the model structure."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "acc = hist.history['accuracy']\n",
    "val_acc = hist.history['val_accuracy']\n",
    "\n",
    "loss = hist.history['loss']\n",
    "val_loss = hist.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.savefig(f'Plots/Batch Size {batch_size} - Epochs {epochs} - Model v{model_version}.png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Plot\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save the model\n",
    "\n",
    "Finally, we save our model in the _Models_ folder with the same information as the plot PNG."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.save(f'Models/Batch Size {batch_size} - Epochs {epochs} - Model v{model_version}.h5')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}